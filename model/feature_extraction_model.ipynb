{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "ad77c151f2568801"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment this, if you are planning to run it in collab (As we did to access high-end GPUs)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "id": "zwNxM3qQbS3k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bcfe903b-cdcb-477a-80e8-281ddc115fa0"
   },
   "id": "zwNxM3qQbS3k",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyperparameters and some constants",
   "id": "10e3aa156bc085d8"
  },
  {
   "metadata": {
    "id": "40999a2ab3a089d8",
    "outputId": "d989a616-5b59-49a6-9359-a6bb4087987c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "decisions_file_path = \"/content/drive/MyDrive/data/sorted_decisions.csv\"\n",
    "images_path = \"/content/drive/MyDrive/data/compressed_images\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Selected device: {device}\")\n"
   ],
   "id": "40999a2ab3a089d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Datasets and dataloaders",
   "id": "3d6fa2d1562fd92f"
  },
  {
   "metadata": {
    "id": "2d5dd9f2a0e9ce98"
   },
   "cell_type": "code",
   "source": [
    "class AssistantDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, labels_file: str, transform=None) -> None:\n",
    "        self.images_dir = images_dir\n",
    "        self.labels = pd.read_csv(labels_file)\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        self.label_encoders = {\n",
    "            \"activity\": LabelEncoder(),\n",
    "            \"hearts\": LabelEncoder(),\n",
    "            \"light_lvl\": LabelEncoder(),\n",
    "            \"in_hand_item\": LabelEncoder(),\n",
    "            \"target_mob\": LabelEncoder(),\n",
    "        }\n",
    "\n",
    "        for col in self.label_encoders:\n",
    "            self.label_encoders[col].fit(self.labels[col])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        image_path = os.path.join(self.images_dir, self.labels.iloc[index, 0])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        heart_region = image.crop((215, 370, 400,  420))\n",
    "        in_hand_region = image.crop((200, 410, 600,  450))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            heart_region = self.transform(heart_region)\n",
    "            in_hand_region = self.transform(in_hand_region)\n",
    "\n",
    "        targets = {\n",
    "            col: self.label_encoders[col].transform([self.labels.iloc[index][col]])[0] for col in self.label_encoders\n",
    "        }\n",
    "\n",
    "        return image, heart_region, in_hand_region, targets\n"
   ],
   "id": "2d5dd9f2a0e9ce98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "325157bd20e02319"
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = AssistantDataset(images_dir=images_path, labels_file=decisions_file_path, transform=transform)\n",
    "train_ds, valid_ds = random_split(ds, [0.8, 0.2])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=4)\n"
   ],
   "id": "325157bd20e02319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculating \"Class Weights\"",
   "id": "1da203e878f47503"
  },
  {
   "metadata": {
    "id": "c5294dbdf509ce82"
   },
   "cell_type": "code",
   "source": [
    "class_weights = {}\n",
    "for col, encoder in ds.label_encoders.items():\n",
    "    labels = encoder.transform(ds.labels[col])\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "    class_weights[col] = torch.tensor(weights, dtype=torch.float).to(device)\n"
   ],
   "id": "c5294dbdf509ce82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model metrics\n",
    "This class actually made for convenience of accessing loss and accuracy functions"
   ],
   "id": "e9fbd31d0af96758"
  },
  {
   "metadata": {
    "id": "7ab6f4582a76c161"
   },
   "cell_type": "code",
   "source": [
    "class ModelMetrics:\n",
    "    def __init__(self, num_classes: dict, class_weights: dict) -> None:\n",
    "        self.loss_fns = {key: nn.CrossEntropyLoss(weight=class_weights[key]) for key in num_classes}\n",
    "\n",
    "    def calculate_loss(self, predictions: dict[torch.Tensor], targets: dict) -> tuple[dict, list]:\n",
    "        total_loss = 0\n",
    "        losses = {}\n",
    "        for key, pred in predictions.items():\n",
    "            target = targets[key].to(pred.device)\n",
    "            loss = self.loss_fns[key](pred, target)\n",
    "            losses[key] = loss.item()\n",
    "            total_loss += loss\n",
    "        return total_loss, losses\n",
    "\n",
    "    def calculate_accuracy(self, predictions: dict[torch.Tensor], targets: dict) -> dict:\n",
    "        accuracies = {}\n",
    "        for key, pred in predictions.items():\n",
    "            target = targets[key].to(pred.device)\n",
    "            acc = (pred.argmax(dim=1) == target).float().mean().item()\n",
    "            accuracies[key] = acc\n",
    "        return accuracies\n"
   ],
   "id": "7ab6f4582a76c161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Models",
   "id": "96772844ae9b0a9d"
  },
  {
   "metadata": {
    "id": "989b7daba54d9455"
   },
   "cell_type": "code",
   "source": [
    "class HeartsInHandClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This model is responsible for detecting in-hand item and player`s hearts.\n",
    "    \"\"\"\n",
    "    def __init__(self, hearts_classes: int, in_hand_classes: int) -> None:\n",
    "        super(HeartsInHandClassifier, self).__init__()\n",
    "\n",
    "        self.resnet_hearts = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet_in_hand = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze first 6 layers in order to preserve some basic learned features\n",
    "        for param in list(self.resnet_hearts.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "        for param in list(self.resnet_in_hand.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.resnet_hearts.fc = nn.Identity()\n",
    "        self.resnet_in_hand.fc = nn.Identity()\n",
    "\n",
    "        self.fc_hearts = nn.Linear(512, hearts_classes)\n",
    "        self.fc_in_hand = nn.Linear(512, in_hand_classes)\n",
    "\n",
    "    def forward(self, heart_region: torch.Tensor, in_hand_region: torch.Tensor) -> dict:\n",
    "        heart_features = self.resnet_hearts(heart_region)\n",
    "        in_hand_features = self.resnet_in_hand(in_hand_region)\n",
    "\n",
    "        hearts_pred = self.fc_hearts(heart_features)\n",
    "        in_hand_pred = self.fc_in_hand(in_hand_features)\n",
    "\n",
    "        return {\"hearts\": hearts_pred, \"in_hand_item\": in_hand_pred}\n",
    "\n",
    "\n",
    "class ImageContextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This model responsible for detecting light lvl, player`s activity and target mob.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: dict) -> None:\n",
    "        super(ImageContextClassifier, self).__init__()\n",
    "        self.cnn = models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze first 6 layers in order to preserve some basic learned features\n",
    "        for param in list(self.cnn.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "        for param in list(self.cnn.parameters())[6:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "        self.fc = nn.ModuleDict({\n",
    "            key: nn.Linear(512, num) for key, num in num_classes.items() if key not in [\"hearts\", \"in_hand_item\"]\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        features = self.cnn(x)\n",
    "        return {key: layer(features) for key, layer in self.fc.items()}\n",
    "\n",
    "\n",
    "# Ensemble function to combine predictions from both models\n",
    "def ensemble_predict(\n",
    "        hearts_model: HeartsInHandClassifier,\n",
    "        context_model: ImageContextClassifier,\n",
    "        heart_region: torch.Tensor,\n",
    "        in_hand_region: torch.Tensor,\n",
    "        full_image: torch.Tensor\n",
    ") -> dict:\n",
    "    hearts_output = hearts_model(heart_region, in_hand_region)\n",
    "    context_output = context_model(full_image)\n",
    "    return {**hearts_output, **context_output}\n"
   ],
   "id": "989b7daba54d9455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training process",
   "id": "e406ea4646ce311d"
  },
  {
   "metadata": {
    "id": "63be620b551d14a3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f59a18e-306d-450e-8e81-be38067df82a"
   },
   "cell_type": "code",
   "source": [
    "# Training Setup\n",
    "num_classes = {col: len(ds.label_encoders[col].classes_) for col in ds.label_encoders}\n",
    "hearts_model = HeartsInHandClassifier(hearts_classes=num_classes[\"hearts\"], in_hand_classes=num_classes[\"in_hand_item\"]).to(device)\n",
    "context_model = ImageContextClassifier(num_classes=num_classes).to(device)\n",
    "\n",
    "metrics = ModelMetrics(num_classes, class_weights)\n",
    "optimizer_hearts = optim.Adam(hearts_model.parameters(), lr=learning_rate)\n",
    "optimizer_context = optim.Adam(context_model.parameters(), lr=learning_rate)\n"
   ],
   "id": "63be620b551d14a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a63c804b301f1bf2"
   },
   "cell_type": "code",
   "source": [
    "def fit(training_accuracies: dict, validation_accuracies: dict, training_losses: list, validation_losses: list, epochs: int) -> None:\n",
    "    for epoch in range(epochs):\n",
    "        hearts_model.train()\n",
    "        context_model.train()\n",
    "\n",
    "        total_loss_train = 0\n",
    "        total_accuracies_train = {key: 0 for key in num_classes}\n",
    "\n",
    "        # Training phase\n",
    "        for images, heart_regions, in_hand_regions, targets in train_dl:\n",
    "            images, heart_regions, in_hand_regions = images.to(device), heart_regions.to(device), in_hand_regions.to(device)\n",
    "            targets = {key: val.to(device) for key, val in targets.items()}\n",
    "\n",
    "            hearts_output = hearts_model(heart_regions, in_hand_regions)\n",
    "            context_output = context_model(images)\n",
    "            predictions = {**hearts_output, **context_output}\n",
    "            loss, batch_losses = metrics.calculate_loss(predictions, targets)\n",
    "\n",
    "            optimizer_hearts.zero_grad()\n",
    "            optimizer_context.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_hearts.step()\n",
    "            optimizer_context.step()\n",
    "\n",
    "\n",
    "            total_loss_train += loss.item()\n",
    "            batch_accuracies = metrics.calculate_accuracy(predictions, targets)\n",
    "            for key in total_accuracies_train:\n",
    "                total_accuracies_train[key] += batch_accuracies[key]\n",
    "\n",
    "        avg_loss = total_loss_train / len(train_dl)\n",
    "        training_losses.append(avg_loss)\n",
    "        avg_train_accuracies = {key: acc / len(train_dl) for key, acc in total_accuracies_train.items()}\n",
    "        for key in training_accuracies:\n",
    "            training_accuracies[key].append(avg_train_accuracies[key])\n",
    "\n",
    "        print(f\"[TRAIN] Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "        print(\"Accuracies:\", avg_train_accuracies)\n",
    "\n",
    "        hearts_model.eval()\n",
    "        context_model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss_valid = 0\n",
    "            total_accuracies_valid = {key: 0 for key in num_classes}\n",
    "\n",
    "            for images, heart_regions, in_hand_regions, targets in valid_dl:\n",
    "                images, heart_regions, in_hand_regions = images.to(device), heart_regions.to(device), in_hand_regions.to(device)\n",
    "                targets = {key: val.to(device) for key, val in targets.items()}\n",
    "\n",
    "                hearts_output = hearts_model(heart_regions, in_hand_regions)\n",
    "                context_output = context_model(images)\n",
    "                predictions = {**hearts_output, **context_output}\n",
    "\n",
    "                loss, batch_losses = metrics.calculate_loss(predictions, targets)\n",
    "                total_loss_valid += loss.item()\n",
    "\n",
    "                batch_accuracies = metrics.calculate_accuracy(predictions, targets)\n",
    "                for key in total_accuracies_valid:\n",
    "                    total_accuracies_valid[key] += batch_accuracies[key]\n",
    "\n",
    "            avg_loss_valid = total_loss_valid / len(valid_dl)\n",
    "            validation_losses.append(avg_loss_valid)\n",
    "            avg_valid_accuracies = {key: acc / len(valid_dl) for key, acc in total_accuracies_valid.items()}\n",
    "            for key in validation_accuracies:\n",
    "                validation_accuracies[key].append(avg_valid_accuracies[key])\n",
    "\n",
    "            print(f\"[VALID] Epoch {epoch+1}/{epochs} - Loss: {avg_loss_valid:.4f}\")\n",
    "            print(\"Accuracies:\", avg_valid_accuracies)\n"
   ],
   "id": "a63c804b301f1bf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e412499dd116adb3",
    "outputId": "ba3667fb-5f57-4176-9de1-ada1d2df3003",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    }
   },
   "cell_type": "code",
   "source": [
    "training_accuracies = {key: [] for key in num_classes}\n",
    "validation_accuracies = {key: [] for key in num_classes}\n",
    "training_losses, validation_losses = [], []\n",
    "\n",
    "fit(\n",
    "    training_accuracies=training_accuracies,\n",
    "    validation_accuracies=validation_accuracies,\n",
    "    training_losses=training_losses,\n",
    "    validation_losses=validation_losses,\n",
    "    epochs=epochs\n",
    ")\n"
   ],
   "id": "e412499dd116adb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4339963b494c9dca"
   },
   "cell_type": "code",
   "source": [
    "hearts_model.cpu()\n",
    "context_model.cpu()\n",
    "torch.save(hearts_model.state_dict(), \"./hearts_model.pth\")\n",
    "torch.save(context_model.state_dict(), \"./context_model.pth\")\n"
   ],
   "id": "4339963b494c9dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Displaying metrics graphs",
   "id": "49038b431b37ae9c"
  },
  {
   "metadata": {
    "id": "8196df3aa4318bbc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "outputId": "7c7becba-b25b-48fb-85d3-fcec7ca8fcb8"
   },
   "cell_type": "code",
   "source": [
    "# Display loss\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses, label=\"Training Loss\", color='steelblue', linestyle='-', linewidth=2)\n",
    "plt.plot(validation_losses, label=\"Validation Loss\", color='darkorange', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=14, labelpad=15)\n",
    "plt.ylabel(\"Loss\", fontsize=14, labelpad=15)\n",
    "plt.title(\"Training and Validation Loss over Epochs\", fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.legend(loc=\"upper right\", fontsize=12, frameon=True, shadow=True)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"training_validation_loss.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "8196df3aa4318bbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "97676f6f5773bcd8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "outputId": "d52a6764-c230-4398-bdb8-d7626613b4aa"
   },
   "cell_type": "code",
   "source": [
    "# Display accuracies V1\n",
    "epochs = list(range(1, len(training_accuracies[next(iter(training_accuracies))]) + 1))\n",
    "\n",
    "for name, accuracies in zip((\"Training\", \"Validation\"), (training_accuracies, validation_accuracies)):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for key, values in accuracies.items():\n",
    "        plt.plot(epochs, values, label=key, linewidth=2)\n",
    "\n",
    "    plt.title(f'{name} accuracy Trends Across Epochs for Each Task', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Epoch', fontsize=14, labelpad=15)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14, labelpad=15)\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\"))\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12, frameon=True, shadow=True)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "    plt.savefig(f\"{name.lower()}_accuracy_trends.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "97676f6f5773bcd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1bba5fdef8c7ce82",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "outputId": "c225b704-5b88-4bca-9d3d-1a81f5dc8204"
   },
   "cell_type": "code",
   "source": [
    "# Display accuracies V2\n",
    "epochs = list(range(1, len(next(iter(training_accuracies.values()))) + 1))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1, len(training_accuracies), figsize=(20, 5), sharey=True)\n",
    "\n",
    "for idx, (key, train_values) in enumerate(training_accuracies.items()):\n",
    "    val_values = validation_accuracies[key]\n",
    "\n",
    "    axs[idx].plot(epochs, train_values, label='Training', color='steelblue', linewidth=2)\n",
    "    axs[idx].plot(epochs, val_values, label='Validation', color='darkorange', linestyle='--', linewidth=2)\n",
    "\n",
    "    axs[idx].set_title(key, fontsize=12, fontweight='bold')\n",
    "    axs[idx].set_xlabel('Epoch', fontsize=10)\n",
    "    if idx == 0:\n",
    "        axs[idx].set_ylabel('Accuracy (%)', fontsize=10)\n",
    "    axs[idx].set_ylim(0, 1)\n",
    "    axs[idx].tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    axs[idx].yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\"))\n",
    "\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='upper left', bbox_to_anchor=(1.05, 1), fontsize=10, shadow=True)\n",
    "plt.tight_layout(pad=3.0)\n",
    "\n",
    "\n",
    "plt.savefig(\"accuracy_trends_subplots.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "1bba5fdef8c7ce82",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
